{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shailendrabhandari/ACIT4630_Advanced_MLandDL/blob/master/EEG-Siamnse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d94db72-46f6-4b44-b0df-b871601b3aae",
      "metadata": {
        "tags": [],
        "id": "1d94db72-46f6-4b44-b0df-b871601b3aae"
      },
      "source": [
        "## Data Files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b0a673-a8e5-461e-9bd2-2c59ecc4730e",
      "metadata": {
        "id": "f8b0a673-a8e5-461e-9bd2-2c59ecc4730e"
      },
      "source": [
        "Here we read the whole data files names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac757685-030e-4c59-b5ac-0ceb1609e648",
      "metadata": {
        "id": "ac757685-030e-4c59-b5ac-0ceb1609e648",
        "outputId": "6f2fa0b4-0f8e-418b-ffe6-c86480e4d8ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1877\n",
            "355\n",
            "95\n",
            "90\n"
          ]
        }
      ],
      "source": [
        "import glob, os\n",
        "train_normal_files = []\n",
        "for file in glob.glob(\"/Users/mohamedr/projects/rules/nmt_scalp_eeg_dataset/normal/train/*edf\"):#*/*/*.edf\"):\n",
        "    train_normal_files.append(file)\n",
        "    #epilespy_files.extend(files)\n",
        "print(len(train_normal_files))\n",
        "\n",
        "train_abnormal_files = []\n",
        "for file in glob.glob(\"/Users/mohamedr/projects/rules/nmt_scalp_eeg_dataset/abnormal/train/*edf\"):#*/*/*.edf\"):\n",
        "    train_abnormal_files.append(file)\n",
        "    #epilespy_files.extend(files)\n",
        "print(len(train_abnormal_files))\n",
        "\n",
        "test_normal_files = []\n",
        "for file in glob.glob(\"/Users/mohamedr/projects/rules/nmt_scalp_eeg_dataset/normal/eval/*edf\"):#*/*/*.edf\"):\n",
        "    test_normal_files.append(file)\n",
        "    #epilespy_files.extend(files)\n",
        "print(len(test_normal_files))\n",
        "\n",
        "test_abnormal_files = []\n",
        "for file in glob.glob(\"/Users/mohamedr/projects/rules/nmt_scalp_eeg_dataset/abnormal/eval/*edf\"):#*/*/*.edf\"):\n",
        "    test_abnormal_files.append(file)\n",
        "    #epilespy_files.extend(files)\n",
        "print(len(test_abnormal_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149a4c79-0983-489c-a48f-ebca4c47b4ee",
      "metadata": {
        "id": "149a4c79-0983-489c-a48f-ebca4c47b4ee"
      },
      "source": [
        "## Reading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52848844-1783-430e-8cc7-be927c6d33c8",
      "metadata": {
        "id": "52848844-1783-430e-8cc7-be927c6d33c8"
      },
      "source": [
        "This function is used to read the whole data to numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f637a76-f527-4c77-a54a-ef4aaa82e781",
      "metadata": {
        "tags": [],
        "id": "6f637a76-f527-4c77-a54a-ef4aaa82e781"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#NMT dataset\n",
        "import mne\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def build_data(raw_data):\n",
        "    \n",
        "    eeg_data = []\n",
        "    for file in raw_data:\n",
        "        data = mne.io.read_raw_edf(file, verbose=False, preload=True)\n",
        "        try:\n",
        "            if data.last_samp > 845:\n",
        "                data.filter(l_freq=1, h_freq=45, verbose=False)\n",
        "                data = mne.make_fixed_length_epochs(data, duration=1, overlap=0, verbose=False)\n",
        "                data = data.get_data()\n",
        "                eeg_data.append(data)\n",
        "        except:\n",
        "            pass\n",
        "    eeg_data = np.vstack(eeg_data)\n",
        "    return eeg_data\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "train_normal_files = random.sample(train_normal_files, 100)\n",
        "train_abnormal_files = random.sample(train_abnormal_files, 100)\n",
        "\n",
        "train_normal_data = build_data(train_normal_files)\n",
        "train_abnormal_data = build_data(train_abnormal_files)\n",
        "test_normal_data = build_data(test_normal_files)\n",
        "test_abnormal_data = build_data(test_abnormal_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf1be122-6f0b-4911-9ab5-5e1cdbd573f2",
      "metadata": {
        "id": "cf1be122-6f0b-4911-9ab5-5e1cdbd573f2",
        "outputId": "9e25cc5f-2f45-4d77-ba85-0915dd6d34f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((70819, 21, 200), (76455, 21, 200), (67090, 21, 200), (64794, 21, 200))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_normal_data.shape, train_abnormal_data.shape, test_normal_data.shape, test_abnormal_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7818cb10-7cd0-4815-8a96-b51a32905944",
      "metadata": {
        "id": "7818cb10-7cd0-4815-8a96-b51a32905944"
      },
      "source": [
        "The next functions are helper functions to standerdize the data and calculate accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd9c70cc-ff9c-40aa-b2e0-42cf40072354",
      "metadata": {
        "id": "fd9c70cc-ff9c-40aa-b2e0-42cf40072354",
        "outputId": "6d11ea51-22db-4b9b-aeab-699faca25284"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mohamedr/opt/anaconda3/envs/xai/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import TransformerMixin,BaseEstimator\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "#https://stackoverflow.com/questions/50125844/how-to-standard-scale-a-3d-matrix\n",
        "class StandardScaler3D(BaseEstimator,TransformerMixin):\n",
        "    #batch, sequence, channels\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def fit(self,X,y=None):\n",
        "        self.scaler.fit(X.reshape(-1, X.shape[2]))\n",
        "        return self\n",
        "\n",
        "    def transform(self,X):\n",
        "        return self.scaler.transform(X.reshape( -1,X.shape[2])).reshape(X.shape)\n",
        "\n",
        "\n",
        "def evaluate_model(model, loss_func, data_iter):\n",
        "    model.eval()\n",
        "    loss_sum, n = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in data_iter:\n",
        "            y_pred = model(x)\n",
        "            y_pred = y_pred.squeeze()\n",
        "            loss = loss_func(y_pred,y)\n",
        "            loss_sum += loss.item()\n",
        "            n += 1\n",
        "        return loss_sum / n\n",
        "\n",
        "\n",
        "def cal_accuracy(model, data_iter):\n",
        "    ytrue = []\n",
        "    ypreds = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in data_iter:\n",
        "            yhat = model(x)\n",
        "            yhat = [0 if i<0.5 else 1 for i in yhat]\n",
        "            ytrue.extend(list(y.numpy()))\n",
        "            ypreds.extend(yhat)\n",
        "\n",
        "    return (accuracy_score(ytrue, ypreds), \n",
        "            confusion_matrix(ytrue, ypreds), \n",
        "            precision_score(ytrue, ypreds), \n",
        "            recall_score(ytrue, ypreds),\n",
        "            f1_score(ytrue, ypreds))\n",
        "\n",
        "def standardize_data(train_features, test_features):\n",
        "    scaler = StandardScaler3D()\n",
        "    train_features = scaler.fit_transform(train_features)\n",
        "    #val_features = scaler.fit_transform(val_features)\n",
        "    test_features = scaler.transform(test_features)\n",
        "    return train_features, test_features\n",
        "\n",
        "\n",
        "def data_loader(features, labels, device, batch_size, shuffle=True):\n",
        "\n",
        "    features = torch.Tensor(features).float().to(device)\n",
        "    labels = torch.Tensor(labels).float().to(device)\n",
        "    data = torch.utils.data.TensorDataset(features, labels)\n",
        "    data_iter = torch.utils.data.DataLoader(data, batch_size, shuffle=shuffle)\n",
        "\n",
        "    return data_iter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cbb58db-58eb-4025-ba83-bc209c426bd7",
      "metadata": {
        "id": "6cbb58db-58eb-4025-ba83-bc209c426bd7"
      },
      "source": [
        "The next cells to standardize the data, build the labels arrays and build arrays for the features and the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f460e009-4f49-4afe-a47a-e9090c40911b",
      "metadata": {
        "id": "f460e009-4f49-4afe-a47a-e9090c40911b"
      },
      "outputs": [],
      "source": [
        "# Standardize the data\n",
        "train_normal_data, test_normal_data = standardize_data(train_normal_data, test_normal_data)\n",
        "train_abnormal_data, test_abnormal_data = standardize_data(train_abnormal_data, test_abnormal_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7459f2e-203e-4bd6-93ea-bc2f06442197",
      "metadata": {
        "id": "c7459f2e-203e-4bd6-93ea-bc2f06442197"
      },
      "outputs": [],
      "source": [
        "# building data labels for the sliding windows of the data \n",
        "test_abnormal_labels=[1 for x in test_abnormal_data]\n",
        "test_normal_labels=[0 for x in test_normal_data]\n",
        "train_abnormal_labels=[1 for x in train_abnormal_data]\n",
        "train_normal_labels=[0 for x in train_normal_data]\n",
        "\n",
        "test_abnormal_labels = np.array(test_abnormal_labels)\n",
        "test_normal_labels = np.array(test_normal_labels)\n",
        "train_abnormal_labels = np.array(train_abnormal_labels)\n",
        "train_normal_labels = np.array(train_normal_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edcafdc6-4008-463b-a9df-f7950dda0850",
      "metadata": {
        "id": "edcafdc6-4008-463b-a9df-f7950dda0850",
        "outputId": "47616701-232c-46ba-eb11-753106eeab48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((76455, 21, 200), (76455,))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_abnormal_data.shape, train_abnormal_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f561150c-bfca-4311-b267-d3a360624bc5",
      "metadata": {
        "tags": [],
        "id": "f561150c-bfca-4311-b267-d3a360624bc5",
        "outputId": "4d0f9c1c-f2e1-423e-dc2c-12c4421c0418"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((147274, 21, 200), (147274,))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_features = np.concatenate((train_normal_data, train_abnormal_data))\n",
        "test_features = np.concatenate((test_normal_data, test_abnormal_data)) \n",
        "train_labels = np.concatenate((train_normal_labels, train_abnormal_labels)) \n",
        "test_labels = np.concatenate((test_normal_labels, test_abnormal_labels)) \n",
        "train_features.shape, train_labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c626ac91-0b04-4bf0-8d24-052c9d920611",
      "metadata": {
        "id": "c626ac91-0b04-4bf0-8d24-052c9d920611"
      },
      "source": [
        "## Stacked LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f082921-66ef-4de0-9677-40019bb573b2",
      "metadata": {
        "id": "0f082921-66ef-4de0-9677-40019bb573b2"
      },
      "source": [
        "Here is the model we used for training on the whole data.\n",
        "\n",
        "Remember as we have the shape of the data as (#num of samples, #num of channels, #number of time points), The LSTM network specs is as follows:\n",
        "- input_size = 200 (num of time points)\n",
        "- hidden_units = number of LSTMs, this can be changed (16, 64 ...etc)\n",
        "- num_layers = number of LSTM layers to stack (number of channels in EEG (21))\n",
        "\n",
        "The LSTM layers are followed by linear layer and then flattened before the sigmoid function to generate outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c78e27e2-7215-4e9b-ab83-4bd636d0e28d",
      "metadata": {
        "id": "c78e27e2-7215-4e9b-ab83-4bd636d0e28d"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class StackedLSTM(nn.Module):\n",
        "    def __init__(self, input_size, num_channels, hidden_units):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_units,\n",
        "            batch_first=True,\n",
        "            num_layers=num_channels,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.linear = nn.Linear(in_features=hidden_units, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "        out = self.linear(hn[0]).flatten()\n",
        "        out = torch.sigmoid(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0360b0fd-ba8c-41e4-bab7-21332a83e228",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "0360b0fd-ba8c-41e4-bab7-21332a83e228",
        "outputId": "7a1e70a7-57dd-4eb2-9d09-38d8281ce92f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Loader....\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'train_features' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m NUM_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Loader....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m train_iter \u001b[38;5;241m=\u001b[39m data_loader(\u001b[43mtrain_features\u001b[49m, train_labels, DEVICE, BATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m test_iter \u001b[38;5;241m=\u001b[39m data_loader(test_features, test_labels, DEVICE, BATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Model....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_features' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "#device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "NUM_EPOCHS = 5\n",
        "    \n",
        "print(\"Data Loader....\")\n",
        "train_iter = data_loader(train_features, train_labels, DEVICE, BATCH_SIZE, shuffle=True)\n",
        "test_iter = data_loader(test_features, test_labels, DEVICE, BATCH_SIZE, shuffle=False)\n",
        "    \n",
        "print(\"Training Model....\")\n",
        "n_chans = 21\n",
        "model=StackedLSTM(200, 21, 64)\n",
        "model.to(DEVICE)\n",
        "loss_func = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    print(\"Epoch\", epoch) \n",
        "    loss_sum, n = 0.0, 0\n",
        "    model.train()\n",
        "    for t, (x, y) in enumerate(tqdm(train_iter)):\n",
        "        y_pred = model(x)\n",
        "        y_pred = y_pred.squeeze()\n",
        "        loss = loss_func(y_pred, y)\n",
        "        loss.backward()\n",
        "        loss_sum += loss.item()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "    val_loss = evaluate_model(model, loss_func, test_iter)\n",
        "    train_accuracy = cal_accuracy(model, train_iter)\n",
        "    val_accuracy = cal_accuracy(model, test_iter)\n",
        "    \n",
        "    print(\"Train loss:\", loss_sum / (t+1), \",Train Accuracy: \", \n",
        "        train_accuracy[0], \",F1: \", \n",
        "        train_accuracy[4], \",Precision: \", \n",
        "        train_accuracy[2], \",Recall: \", \n",
        "        train_accuracy[3])\n",
        "    print(\"Val loss:\", val_loss, \", Val Accuracy: \", \n",
        "        val_accuracy[0], \",F1: \", \n",
        "        val_accuracy[4], \",Precision: \", \n",
        "        val_accuracy[2], \",Recall: \", \n",
        "        val_accuracy[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a06c0c6-97ca-4f7f-b23c-78b2f417a25a",
      "metadata": {
        "id": "2a06c0c6-97ca-4f7f-b23c-78b2f417a25a"
      },
      "source": [
        "It is suitable here to view the results in graphs, chanegs of loss function over time and changes in accuracies too. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1c70a6a-3cfd-4879-a660-4e896ffc5bdb",
      "metadata": {
        "id": "f1c70a6a-3cfd-4879-a660-4e896ffc5bdb",
        "outputId": "efecc82a-df47-4042-86a6-892273f701f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.7424479087683116,\n",
              " array([[50158, 16932],\n",
              "        [17035, 47759]]),\n",
              " 0.7382634369541358,\n",
              " 0.7370898539988271,\n",
              " 0.7376761787079585)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ae64977-204a-4a45-b978-2c09da48925f",
      "metadata": {
        "id": "1ae64977-204a-4a45-b978-2c09da48925f"
      },
      "source": [
        "Until now, we were just training on large subsets of the data which achieved this accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc46b473-1d48-44f0-82de-6b62de7d4ceb",
      "metadata": {
        "id": "bc46b473-1d48-44f0-82de-6b62de7d4ceb"
      },
      "source": [
        "# Working with 3 data samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1662c2a8-d070-4bc6-9f9a-70293db5eddd",
      "metadata": {
        "id": "1662c2a8-d070-4bc6-9f9a-70293db5eddd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "np.random.seed(42)\n",
        "\n",
        "train_normal_files = random.sample(train_normal_files, 3)\n",
        "train_abnormal_files = random.sample(train_abnormal_files, 3)\n",
        "\n",
        "train_normal_data = build_data(train_normal_files)\n",
        "train_abnormal_data = build_data(train_abnormal_files)\n",
        "test_normal_data = build_data(test_normal_files)\n",
        "test_abnormal_data = build_data(test_abnormal_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad775909-993d-4478-83b4-bd698a63243b",
      "metadata": {
        "id": "ad775909-993d-4478-83b4-bd698a63243b"
      },
      "outputs": [],
      "source": [
        "# Standardize the data\n",
        "train_normal_data, test_normal_data = standardize_data(train_normal_data, test_normal_data)\n",
        "train_abnormal_data, test_abnormal_data = standardize_data(train_abnormal_data, test_abnormal_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3e37cdd-f060-489d-b017-3c0da2f3caf0",
      "metadata": {
        "id": "d3e37cdd-f060-489d-b017-3c0da2f3caf0"
      },
      "outputs": [],
      "source": [
        "# building data labels for the sliding windows of the data \n",
        "test_abnormal_labels=[1 for x in test_abnormal_data]\n",
        "test_normal_labels=[0 for x in test_normal_data]\n",
        "train_abnormal_labels=[1 for x in train_abnormal_data]\n",
        "train_normal_labels=[0 for x in train_normal_data]\n",
        "\n",
        "test_abnormal_labels = np.array(test_abnormal_labels)\n",
        "test_normal_labels = np.array(test_normal_labels)\n",
        "train_abnormal_labels = np.array(train_abnormal_labels)\n",
        "train_normal_labels = np.array(train_normal_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97797b6e-22ca-48e2-83ca-a56991d8e2eb",
      "metadata": {
        "id": "97797b6e-22ca-48e2-83ca-a56991d8e2eb",
        "outputId": "f0514b44-d4ea-4d55-b9d6-cb5d24114666"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((4007, 21, 200), (4007,))"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_features = np.concatenate((train_normal_data, train_abnormal_data))\n",
        "test_features = np.concatenate((test_normal_data, test_abnormal_data)) \n",
        "train_labels = np.concatenate((train_normal_labels, train_abnormal_labels)) \n",
        "test_labels = np.concatenate((test_normal_labels, test_abnormal_labels)) \n",
        "train_features.shape, train_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08463a03-0478-4c7c-8665-d836f69b6d79",
      "metadata": {
        "id": "08463a03-0478-4c7c-8665-d836f69b6d79",
        "outputId": "f0afa986-ca1d-4833-b507-a66fe2d69e82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((131884, 21, 200), (131884,))"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_features.shape, test_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81dde79e-9590-4e9c-98be-10ba7c3b6b42",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "81dde79e-9590-4e9c-98be-10ba7c3b6b42",
        "outputId": "c18629ba-8c65-4e0b-9008-14ab9772cb2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Loader....\n",
            "Training Model....\n",
            "Epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:56<00:00,  1.77s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.6851712372153997 ,Train Accuracy:  0.6181682056401298 ,F1:  0.3277680140597539 ,Precision:  0.7803347280334728 ,Recall:  0.20745272525027808\n",
            "Val loss: 0.6921455461231976 , Val Accuracy:  0.5148918746777471 ,F1:  0.1870234827691369 ,Precision:  0.5293482952093224 ,Recall:  0.11357533104917122\n",
            "Epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|█████████████████████████                                                                           | 8/32 [00:16<00:49,  2.05s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(y_pred, y)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/xai/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/xai/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "#device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "NUM_EPOCHS = 5\n",
        "    \n",
        "print(\"Data Loader....\")\n",
        "train_iter = data_loader(train_features, train_labels, DEVICE, BATCH_SIZE, shuffle=True)\n",
        "test_iter = data_loader(test_features, test_labels, DEVICE, BATCH_SIZE, shuffle=False)\n",
        "    \n",
        "print(\"Training Model....\")\n",
        "n_chans = 21\n",
        "model=StackedLSTM(200, 21, 64)\n",
        "model.to(DEVICE)\n",
        "loss_func = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    print(\"Epoch\", epoch) \n",
        "    loss_sum, n = 0.0, 0\n",
        "    model.train()\n",
        "    for t, (x, y) in enumerate(tqdm(train_iter)):\n",
        "        y_pred = model(x)\n",
        "        y_pred = y_pred.squeeze()\n",
        "        loss = loss_func(y_pred, y)\n",
        "        loss.backward()\n",
        "        loss_sum += loss.item()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "    val_loss = evaluate_model(model, loss_func, test_iter)\n",
        "    train_accuracy = cal_accuracy(model, train_iter)\n",
        "    \n",
        "    # This is a bad practice, using test data for validation\n",
        "    # need to fix it\n",
        "    val_accuracy = cal_accuracy(model, test_iter)\n",
        "    \n",
        "    print(\"Train loss:\", loss_sum / (t+1), \",Train Accuracy: \", \n",
        "        train_accuracy[0], \",F1: \", \n",
        "        train_accuracy[4], \",Precision: \", \n",
        "        train_accuracy[2], \",Recall: \", \n",
        "        train_accuracy[3])\n",
        "    print(\"Val loss:\", val_loss, \", Val Accuracy: \", \n",
        "        val_accuracy[0], \",F1: \", \n",
        "        val_accuracy[4], \",Precision: \", \n",
        "        val_accuracy[2], \",Recall: \", \n",
        "        val_accuracy[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eae9b34f-75b9-4b74-b9fb-40883c3c1e8a",
      "metadata": {
        "id": "eae9b34f-75b9-4b74-b9fb-40883c3c1e8a",
        "outputId": "0a8b53ef-4dcd-43e1-e147-3c5a6d007057"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.5148918746777471,\n",
              " array([[60547,  6543],\n",
              "        [57435,  7359]]),\n",
              " 0.5293482952093224,\n",
              " 0.11357533104917122,\n",
              " 0.1870234827691369)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1edf39a9-7f71-4f8b-b6ab-4d3b25415e50",
      "metadata": {
        "id": "1edf39a9-7f71-4f8b-b6ab-4d3b25415e50"
      },
      "source": [
        "# Siamese Network\n",
        "\n",
        "The next cell is LSTM-Siamese Network. The network aims to get two latent represnatations of the two samples using the LSTM we used earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0762c02c-d0aa-451a-87ea-0f8d3c28d2d5",
      "metadata": {
        "id": "0762c02c-d0aa-451a-87ea-0f8d3c28d2d5",
        "outputId": "dd525716-1d33-4ee0-fe2e-a146493be0a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Siamese(\n",
              "  (lstm): LSTM(200, 64, num_layers=21, batch_first=True, bidirectional=True)\n",
              "  (linear): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 206,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class Siamese(nn.Module):\n",
        "    def __init__(self, input_size, num_channels, hidden_units):\n",
        "        super(Siamese, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_units,\n",
        "            batch_first=True,\n",
        "            num_layers=num_channels,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.linear = nn.Linear(in_features=hidden_units, out_features=hidden_units)\n",
        "        self.out = nn.Linear(in_features=hidden_units, out_features=1)\n",
        "        \n",
        "        self.normal_class_representation = torch.zeros(hidden_units)\n",
        "        self.abnormal_class_representation = torch.zeros(hidden_units)\n",
        "        \n",
        "    def forward_one(self, x):\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "        x = self.linear(hn[0])\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        anchor, pos, neg = x[:, 0, :, :], x[:, 1, :, :], x[:, 2, :, :]\n",
        "        anchor = self.forward_one(anchor)\n",
        "        pos = self.forward_one(pos)\n",
        "        neg = self.forward_one(neg)\n",
        "        #dis = torch.abs(out1 - out2)\n",
        "        #out = self.out(dis)\n",
        "        #out = torch.sigmoid(out)\n",
        "        return anchor, pos, neg\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    net = Siamese(200, 21, 64)\n",
        "    # 200 is number of time points\n",
        "    # 21 number of channels\n",
        "    # 64 number of LTSM units\n",
        "    \n",
        "net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a97ac037-59f2-453a-ac09-dfa277b3d51e",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "a97ac037-59f2-453a-ac09-dfa277b3d51e",
        "outputId": "9fbfee86-f1c6-4793-c125-afde37908642"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Siamese.forward() takes 2 positional arguments but 4 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [208]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m x2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m      3\u001b[0m x3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m anchor, pos, neg \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m triplet_loss(anchor, pos, neg)\n\u001b[1;32m      7\u001b[0m output\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/xai/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "\u001b[0;31mTypeError\u001b[0m: Siamese.forward() takes 2 positional arguments but 4 were given"
          ]
        }
      ],
      "source": [
        "x1 = torch.Tensor(1, 21, 200)\n",
        "x2 = torch.Tensor(1, 21, 200)\n",
        "x3 = torch.Tensor(1, 21, 200)\n",
        "\n",
        "anchor, pos, neg = net(x1, x2, x3)\n",
        "output = triplet_loss(anchor, pos, neg)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dadb9c79-1685-4972-a641-a1d866a73151",
      "metadata": {
        "id": "dadb9c79-1685-4972-a641-a1d866a73151"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "np.random.seed(42)\n",
        "#build data\n",
        "train_normal_files = random.sample(train_normal_files, 5)\n",
        "train_abnormal_files = random.sample(train_abnormal_files, 5)\n",
        "train_normal_data = build_data(train_normal_files)\n",
        "train_abnormal_data = build_data(train_abnormal_files)\n",
        "test_normal_data = build_data(test_normal_files)\n",
        "test_abnormal_data = build_data(test_abnormal_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af1ae3c4-89a3-493e-9d8c-26a47d909602",
      "metadata": {
        "id": "af1ae3c4-89a3-493e-9d8c-26a47d909602"
      },
      "outputs": [],
      "source": [
        "# Standardize the data\n",
        "train_normal_data, test_normal_data = standardize_data(train_normal_data, test_normal_data)\n",
        "train_abnormal_data, test_abnormal_data = standardize_data(train_abnormal_data, test_abnormal_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d70cec-4fab-48fe-b271-e15f20f4978c",
      "metadata": {
        "id": "25d70cec-4fab-48fe-b271-e15f20f4978c",
        "outputId": "a758bfef-a9df-4626-a3a4-e681d5522e43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 1, 21, 200)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(4, 3, 21, 200)"
            ]
          },
          "execution_count": 280,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.random.rand(4, 1, 21, 200)\n",
        "#print(x.shape)\n",
        "x = np.repeat(x, 3, axis=1)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "165ea922-308e-46e2-867a-77e0e582106b",
      "metadata": {
        "id": "165ea922-308e-46e2-867a-77e0e582106b",
        "outputId": "1409ef4e-d264-4c5a-dc81-b8018f385b0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((131884, 3, 21, 200), (131884,))"
            ]
          },
          "execution_count": 281,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_labels = [0 for i in test_normal_data]+[1 for i in test_abnormal_data]\n",
        "test_labels = np.array(test_labels)\n",
        "test_data = np.concatenate((test_normal_data, test_abnormal_data))\n",
        "test_data = test_data.reshape(131884,1,  21, 200)\n",
        "test_data = np.repeat(test_data, 3, axis=1)\n",
        "test_data.shape, test_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c267a760-bb7f-4994-80d3-23f5c0e24a0a",
      "metadata": {
        "tags": [],
        "id": "c267a760-bb7f-4994-80d3-23f5c0e24a0a"
      },
      "outputs": [],
      "source": [
        "train_data = []\n",
        "train_labels = []\n",
        "train_features = np.concatenate((train_normal_data, train_abnormal_data))\n",
        "\n",
        "for i in range(4000):\n",
        "    anchor = random.sample(range(train_features.shape[0]), 1)\n",
        "    anchor = anchor[0]\n",
        "    if anchor > len(train_normal_data):\n",
        "        anchor = train_features[anchor]\n",
        "        pos = random.sample(range(len(train_abnormal_data)), 1)\n",
        "        pos = pos[0]\n",
        "        pos = train_abnormal_data[pos]\n",
        "        neg = random.sample(range(len(train_normal_data)), 1)\n",
        "        neg = neg[0]\n",
        "        neg = train_normal_data[neg]\n",
        "        train_data.append([anchor, pos, neg]) \n",
        "        train_labels.append(1)\n",
        "    else:\n",
        "        anchor = train_features[anchor]\n",
        "        pos = random.sample(range(len(train_normal_data)), 1)\n",
        "        pos = pos[0]\n",
        "        pos = train_normal_data[pos]\n",
        "        neg = random.sample(range(len(train_abnormal_data)), 1)\n",
        "        neg = neg[0]\n",
        "        neg = train_abnormal_data[neg]\n",
        "        train_data.append([anchor, pos, neg]) \n",
        "        train_labels.append(0) #0 is label\n",
        "    \n",
        "train_data = np.array(train_data)\n",
        "train_labels = np.array(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82473a6c-2b2c-457f-a594-dab4d4fc967f",
      "metadata": {
        "id": "82473a6c-2b2c-457f-a594-dab4d4fc967f",
        "outputId": "6b2b8e3e-b1b1-4142-9613-ebd3cd47c512"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((4000, 3, 21, 200), (4000,))"
            ]
          },
          "execution_count": 215,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.shape, train_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d4ab4f-23af-44fb-8a3e-665f6a6cd79e",
      "metadata": {
        "tags": [],
        "id": "b4d4ab4f-23af-44fb-8a3e-665f6a6cd79e",
        "outputId": "f736d42d-7f2c-4fa1-fe87-1d00f13f162d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Loader....\n",
            "Training Model....\n",
            "Epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:55<00:00,  5.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31.357930719852448\n",
            "Epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:49<00:00,  5.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29.299343585968018\n",
            "Epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:49<00:00,  5.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27.493416965007782\n",
            "Epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [04:04<00:00,  7.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20.50201240181923\n",
            "Epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [03:40<00:00,  6.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.790253639221191\n",
            "Epoch 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [04:01<00:00,  7.54s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.591545760631561\n",
            "Epoch 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [04:02<00:00,  7.57s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.252599686384201\n",
            "Epoch 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [03:57<00:00,  7.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.052323017269373\n",
            "Epoch 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [03:57<00:00,  7.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0050703454762697\n",
            "Epoch 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [03:44<00:00,  7.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.6373741645365953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "#device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "NUM_EPOCHS = 10\n",
        "    \n",
        "print(\"Data Loader....\")\n",
        "train_iter = data_loader(train_data, train_labels, DEVICE, BATCH_SIZE, shuffle=True)\n",
        "test_iter = data_loader(test_data, test_labels, DEVICE, BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "print(\"Training Model....\")\n",
        "n_chans = 21\n",
        "model=Siamese(200, 21, 64)\n",
        "model.to(DEVICE)\n",
        "loss_func = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    num_y = 0\n",
        "    print(\"Epoch\", epoch) \n",
        "    loss_sum, n = 0.0, 0\n",
        "    model.train()\n",
        "    for t, (x, y)  in enumerate(tqdm(train_iter)):\n",
        "        anchor, pos, neg = model(x)\n",
        "        #y_pred = y_pred.squeeze()\n",
        "        loss = loss_func(anchor, pos, neg)\n",
        "        loss.backward()\n",
        "        loss_sum += loss.item()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #Here we save class representation within the model\n",
        "        if epoch == NUM_EPOCHS:\n",
        "            for i, lab in enumerate(list(y)):\n",
        "                num_y += 1 \n",
        "                if lab == 0:\n",
        "                    model.normal_class_representation += pos[i]\n",
        "                    model.abnormal_class_representation += neg[i]\n",
        "                else:\n",
        "                    model.normal_class_representation += neg[i]\n",
        "                    model.abnormal_class_representation += pos[i]\n",
        "            model.normal_class_representation = model.normal_class_representation / num_y\n",
        "            model.abnormal_class_representation = model.abnormal_class_representation / num_y\n",
        "                                \n",
        "    print(loss_sum)\n",
        "    #val_loss = evaluate_model(model, loss_func, test_iter)\n",
        "    #train_accuracy = cal_accuracy(model, train_iter)\n",
        "    \n",
        "    # This is a bad practice, using test data for validation\n",
        "    # need to fix it\n",
        "    #val_accuracy = cal_accuracy(model, test_iter)\n",
        "    \n",
        "    #print(\"Train loss:\", loss_sum / (t+1), \",Train Accuracy: \", \n",
        "    #    train_accuracy[0], \",F1: \", \n",
        "    #    train_accuracy[4], \",Precision: \", \n",
        "    #    train_accuracy[2], \",Recall: \", \n",
        "    #    train_accuracy[3])\n",
        "    #print(\"Val loss:\", val_loss, \", Val Accuracy: \", \n",
        "    #    val_accuracy[0], \",F1: \", \n",
        "    #    val_accuracy[4], \",Precision: \", \n",
        "    #    val_accuracy[2], \",Recall: \", \n",
        "    #    val_accuracy[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2212f88-90a7-4e3f-94d1-9cd57981e76c",
      "metadata": {
        "id": "d2212f88-90a7-4e3f-94d1-9cd57981e76c"
      },
      "outputs": [],
      "source": [
        "def cal_accuracy(model, data_iter):\n",
        "    ytrue = []\n",
        "    ypreds = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(data_iter):\n",
        "            yhat = model(x)\n",
        "            anchor, pos, neg = model(x)\n",
        "            anchor = anchor.detach().numpy()\n",
        "            pos = pos.detach().numpy()\n",
        "            neg = neg.detach().numpy()\n",
        "            \n",
        "            dist2pos = np.linalg.norm(anchor - pos)\n",
        "            dist2neg = np.linalg.norm(anchor - neg)\n",
        "            \n",
        "            yhat = [1 if dist2pos <= dist2neg else 0]\n",
        "            ytrue.extend(list(y.numpy()))\n",
        "            ypreds.extend(yhat)\n",
        "    \n",
        "    return (accuracy_score(ytrue, ypreds), \n",
        "            confusion_matrix(ytrue, ypreds), \n",
        "            precision_score(ytrue, ypreds), \n",
        "            recall_score(ytrue, ypreds),\n",
        "            f1_score(ytrue, ypreds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6f84703-7785-42e3-b516-d5dacfa1ace3",
      "metadata": {
        "id": "c6f84703-7785-42e3-b516-d5dacfa1ace3"
      },
      "outputs": [],
      "source": [
        "test_iter = data_loader(test_data, test_labels, DEVICE, BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "172b28e5-9758-43f8-91ff-c7b01fd87f6d",
      "metadata": {
        "tags": [],
        "id": "172b28e5-9758-43f8-91ff-c7b01fd87f6d",
        "outputId": "fe35e697-6645-4e74-b186-7d4e8cd9ef52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [52:12<00:00,  3.04s/it]\n"
          ]
        }
      ],
      "source": [
        "ytrue = []\n",
        "ypreds = []\n",
        "with torch.no_grad():\n",
        "    for x, y in tqdm(test_iter):\n",
        "        anchor, pos, neg = model(x)\n",
        "        anchor = anchor.detach().numpy()\n",
        "        abnormal_rep =  model.abnormal_class_representation.detach().numpy()\n",
        "        normal_rep =  model.normal_class_representation.detach().numpy()\n",
        "        \n",
        "        ytrue.extend(y)\n",
        "        for anchor_sample in anchor:\n",
        "            dist2normal = np.linalg.norm(anchor_sample - normal_rep)\n",
        "            dist2abnormal = np.linalg.norm(anchor_sample - abnormal_rep)\n",
        "            if dist2normal >= dist2abnormal:\n",
        "                yhat = 1\n",
        "            else:\n",
        "                yhat = 0\n",
        "            ypreds.append(yhat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292dffdf-d932-47f1-bd72-0ee020713f32",
      "metadata": {
        "id": "292dffdf-d932-47f1-bd72-0ee020713f32",
        "outputId": "b116b33d-34d7-4975-86db-77a4b84b420c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.7584240696369536,\n",
              " array([[36472, 30618],\n",
              "        [ 1242, 63552]]))"
            ]
          },
          "execution_count": 284,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(ytrue, ypreds), confusion_matrix(ytrue, ypreds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f75ad8e4-e044-499a-9896-6aab4f750c14",
      "metadata": {
        "id": "f75ad8e4-e044-499a-9896-6aab4f750c14"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1960d8c6-04db-4e42-8358-2adb9d3e4180",
      "metadata": {
        "id": "1960d8c6-04db-4e42-8358-2adb9d3e4180"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4374cc4-3793-4615-8cb0-d3b5c651ea41",
      "metadata": {
        "id": "e4374cc4-3793-4615-8cb0-d3b5c651ea41"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e650a4be-59ef-4ad4-b2a4-220ca21b3650",
      "metadata": {
        "id": "e650a4be-59ef-4ad4-b2a4-220ca21b3650"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e0eb39e-9fa2-43fb-abef-202dd45c421e",
      "metadata": {
        "id": "6e0eb39e-9fa2-43fb-abef-202dd45c421e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ee5276b-7d75-46cd-a8df-c0597cba70ff",
      "metadata": {
        "id": "8ee5276b-7d75-46cd-a8df-c0597cba70ff"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}